{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102333f-4fb4-4312-9f3a-082960ea4de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in /home/somu/.local/lib/python3.10/site-packages (from torchvision) (1.25.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
      "Collecting torch==2.2.0 (from torchvision)\n",
      "  Using cached torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/somu/.local/lib/python3.10/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: filelock in /home/somu/.local/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchvision) (4.9.0)\n",
      "Collecting sympy (from torch==2.2.0->torchvision)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /home/somu/.local/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/somu/.local/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (3.0.3)\n",
      "Requirement already satisfied: fsspec in /home/somu/.local/lib/python3.10/site-packages (from torch==2.2.0->torchvision) (2023.12.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0->torchvision)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0->torchvision)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0->torchvision)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0->torchvision)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0->torchvision)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0->torchvision)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0->torchvision)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0->torchvision)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0->torchvision)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0->torchvision)\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0->torchvision)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.2.0 (from torch==2.2.0->torchvision)\n",
      "  Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/somu/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->torchvision) (12.2.140)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0->torchvision) (2.1.3)\n",
      "Collecting mpmath>=0.19 (from sympy->torch==2.2.0->torchvision)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m895.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/731.7 MB\u001b[0m \u001b[31m934.3 kB/s\u001b[0m eta \u001b[36m0:11:24\u001b[0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/731.7 MB\u001b[0m \u001b[31m956.2 kB/s\u001b[0m eta \u001b[36m0:12:36\u001b[0mK   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/731.7 MB\u001b[0m \u001b[31m956.2 kB/s\u001b[0m eta \u001b[36m0:12:36\u001b[0m�━━━━━━━━━━\u001b[0m \u001b[32m10.3/731.7 MB\u001b[0m \u001b[31m471.6 kB/s\u001b[0m eta \u001b[36m0:25:30\u001b[0m1m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/731.7 MB\u001b[0m \u001b[31m471.6 kB/s\u001b[0m eta \u001b[36m0:25:30\u001b[0m��━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.4/731.7 MB\u001b[0m \u001b[31m343.3 kB/s\u001b[0m eta \u001b[36m0:34:38\u001b[0m[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.5/731.7 MB\u001b[0m \u001b[31m374.7 kB/s\u001b[0m eta \u001b[36m0:31:33\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install torchvision \n",
    "# !pip install imutils\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imutils\n",
    "import matplotlib.image as mpimg\n",
    "from collections import OrderedDict\n",
    "from skimage import io, transform\n",
    "from math import *\n",
    "import xml.etree.ElementTree as ET \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa15b4aa-20ef-4dcc-8afe-b1ea68085f48",
   "metadata": {},
   "source": [
    "### Downloading the DLIB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edceccd-af75-4a72-b22e-f07f94edafa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "if not os.path.exists('/content/ibug_300W_large_face_landmark_dataset'):\n",
    "    !wget http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz\n",
    "    !tar -xvzf 'ibug_300W_large_face_landmark_dataset.tar.gz'    \n",
    "    !rm -r 'ibug_300W_large_face_landmark_dataset.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4de7d0-1c60-46a7-8fcc-3ea05f7975fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e751d1d-2e80-40fe-9a14-dadde53063af",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('ibug_300W_large_face_landmark_dataset/helen/trainset/100032540_1.pts')\n",
    "points = file.readlines()[3:-1]\n",
    "\n",
    "landmarks = []\n",
    "\n",
    "for point in points:\n",
    "    x,y = point.split(' ')\n",
    "    landmarks.append([floor(float(x)), floor(float(y[:-1]))])\n",
    "\n",
    "landmarks = np.array(landmarks)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(mpimg.imread('ibug_300W_large_face_landmark_dataset/helen/trainset/100032540_1.jpg'))\n",
    "plt.scatter(landmarks[:,0], landmarks[:,1], s = 5, c = 'g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb25465-39a5-4b48-a144-034803a395c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transforms():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def rotate(self, image, landmarks, angle):\n",
    "        angle = random.uniform(-angle, +angle)\n",
    "\n",
    "        transformation_matrix = torch.tensor([\n",
    "            [+cos(radians(angle)), -sin(radians(angle))], \n",
    "            [+sin(radians(angle)), +cos(radians(angle))]\n",
    "        ])\n",
    "\n",
    "        image = imutils.rotate(np.array(image), angle)\n",
    "\n",
    "        landmarks = landmarks - 0.5\n",
    "        new_landmarks = np.matmul(landmarks, transformation_matrix)\n",
    "        new_landmarks = new_landmarks + 0.5\n",
    "        return Image.fromarray(image), new_landmarks\n",
    "\n",
    "    def resize(self, image, landmarks, img_size):\n",
    "        image = TF.resize(image, img_size)\n",
    "        return image, landmarks\n",
    "\n",
    "    def color_jitter(self, image, landmarks):\n",
    "        color_jitter = transforms.ColorJitter(brightness=0.3, \n",
    "                                              contrast=0.3,\n",
    "                                              saturation=0.3, \n",
    "                                              hue=0.1)\n",
    "        image = color_jitter(image)\n",
    "        return image, landmarks\n",
    "\n",
    "    def crop_face(self, image, landmarks, crops):\n",
    "        left = int(crops['left'])\n",
    "        top = int(crops['top'])\n",
    "        width = int(crops['width'])\n",
    "        height = int(crops['height'])\n",
    "\n",
    "        image = TF.crop(image, top, left, height, width)\n",
    "\n",
    "        img_shape = np.array(image).shape\n",
    "        landmarks = torch.tensor(landmarks) - torch.tensor([[left, top]])\n",
    "        landmarks = landmarks / torch.tensor([img_shape[1], img_shape[0]])\n",
    "        return image, landmarks\n",
    "\n",
    "    def __call__(self, image, landmarks, crops):\n",
    "        image = Image.fromarray(image)\n",
    "        image, landmarks = self.crop_face(image, landmarks, crops)\n",
    "        image, landmarks = self.resize(image, landmarks, (224, 224))\n",
    "        image, landmarks = self.color_jitter(image, landmarks)\n",
    "        image, landmarks = self.rotate(image, landmarks, angle=10)\n",
    "        \n",
    "        image = TF.to_tensor(image)\n",
    "        image = TF.normalize(image, [0.5], [0.5])\n",
    "        return image, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f99628-c871-4a5f-be5a-403cd241bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceLandmarksDataset(Dataset):\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "\n",
    "        tree = ET.parse('ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml')\n",
    "        root = tree.getroot()\n",
    "\n",
    "        self.image_filenames = []\n",
    "        self.landmarks = []\n",
    "        self.crops = []\n",
    "        self.transform = transform\n",
    "        self.root_dir = 'ibug_300W_large_face_landmark_dataset'\n",
    "        \n",
    "        for filename in root[2]:\n",
    "            self.image_filenames.append(os.path.join(self.root_dir, filename.attrib['file']))\n",
    "\n",
    "            self.crops.append(filename[0].attrib)\n",
    "\n",
    "            landmark = []\n",
    "            for num in range(68):\n",
    "                x_coordinate = int(filename[0][num].attrib['x'])\n",
    "                y_coordinate = int(filename[0][num].attrib['y'])\n",
    "                landmark.append([x_coordinate, y_coordinate])\n",
    "            self.landmarks.append(landmark)\n",
    "\n",
    "        self.landmarks = np.array(self.landmarks).astype('float32')     \n",
    "\n",
    "        assert len(self.image_filenames) == len(self.landmarks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.image_filenames[index], 0)\n",
    "        landmarks = self.landmarks[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            image, landmarks = self.transform(image, landmarks, self.crops[index])\n",
    "\n",
    "        landmarks = landmarks - 0.5\n",
    "\n",
    "        return image, landmarks\n",
    "\n",
    "dataset = FaceLandmarksDataset(Transforms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1805b77b-8d0e-48cb-95dc-7088549b2f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, landmarks = dataset[0]\n",
    "landmarks = (landmarks + 0.5) * 224\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image.numpy().squeeze(), cmap='gray');\n",
    "plt.scatter(landmarks[:,0], landmarks[:,1], s=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec13b0f3-7160-4775-910d-4dd0b5471be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into validation and test sets\n",
    "len_valid_set = int(0.1*len(dataset))\n",
    "len_train_set = len(dataset) - len_valid_set\n",
    "\n",
    "print(\"The length of Train set is {}\".format(len_train_set))\n",
    "print(\"The length of Valid set is {}\".format(len_valid_set))\n",
    "\n",
    "train_dataset , valid_dataset,  = torch.utils.data.random_split(dataset , [len_train_set, len_valid_set])\n",
    "\n",
    "# shuffle and batch the datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=8, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc6769-0dd0-455f-9677-90f00ab398f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, landmarks = next(iter(train_loader))\n",
    "\n",
    "print(images.shape)\n",
    "print(landmarks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5688b759-0c56-46a1-9b5d-7a6d7aa77e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,num_classes=136):\n",
    "        super().__init__()\n",
    "        self.model_name='resnet18'\n",
    "        self.model=models.resnet18()\n",
    "        self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.fc=nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491837b7-e960-4cb5-814b-8c1caea152c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_overwrite(step, total_step, loss, operation):\n",
    "    sys.stdout.write('\\r')\n",
    "    if operation == 'train':\n",
    "        sys.stdout.write(\"Train Steps: %d/%d  Loss: %.4f \" % (step, total_step, loss))   \n",
    "    else:\n",
    "        sys.stdout.write(\"Valid Steps: %d/%d  Loss: %.4f \" % (step, total_step, loss))\n",
    "        \n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb90f7-22c3-48a2-aba9-e9fc8962bf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/content'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "torch.save(network.state_dict(), '/content/face_landmarks.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227da072-6420-4cdf-a20a-b80efb816677",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "network = Network()\n",
    "network.cuda()    \n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.0001)\n",
    "\n",
    "loss_min = np.inf\n",
    "num_epochs = 10\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    \n",
    "    loss_train = 0\n",
    "    loss_valid = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    network.train()\n",
    "    for step in range(1,len(train_loader)+1):\n",
    "    \n",
    "        images, landmarks = next(iter(train_loader))\n",
    "        \n",
    "        images = images.cuda()\n",
    "        landmarks = landmarks.view(landmarks.size(0),-1).cuda() \n",
    "        \n",
    "        predictions = network(images)\n",
    "        \n",
    "        # clear all the gradients before calculating them\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # find the loss for the current step\n",
    "        loss_train_step = criterion(predictions, landmarks)\n",
    "        \n",
    "        # calculate the gradients\n",
    "        loss_train_step.backward()\n",
    "        \n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_train += loss_train_step.item()\n",
    "        running_loss = loss_train/step\n",
    "        \n",
    "        print_overwrite(step, len(train_loader), running_loss, 'train')\n",
    "        \n",
    "    network.eval() \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for step in range(1,len(valid_loader)+1):\n",
    "            \n",
    "            images, landmarks = next(iter(valid_loader))\n",
    "        \n",
    "            images = images.cuda()\n",
    "            landmarks = landmarks.view(landmarks.size(0),-1).cuda()\n",
    "        \n",
    "            predictions = network(images)\n",
    "\n",
    "            # find the loss for the current step\n",
    "            loss_valid_step = criterion(predictions, landmarks)\n",
    "\n",
    "            loss_valid += loss_valid_step.item()\n",
    "            running_loss = loss_valid/step\n",
    "\n",
    "            print_overwrite(step, len(valid_loader), running_loss, 'valid')\n",
    "    \n",
    "    loss_train /= len(train_loader)\n",
    "    loss_valid /= len(valid_loader)\n",
    "    \n",
    "    print('\\n--------------------------------------------------')\n",
    "    print('Epoch: {}  Train Loss: {:.4f}  Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid))\n",
    "    print('--------------------------------------------------')\n",
    "    \n",
    "    if loss_valid < loss_min:\n",
    "        loss_min = loss_valid\n",
    "        torch.save(network.state_dict(), '/content/face_landmarks.pth') \n",
    "        print(\"\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\".format(loss_min, epoch, num_epochs))\n",
    "        print('Model Saved\\n')\n",
    "print('Training Complete')\n",
    "print(\"Total Elapsed Time : {} s\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb86789-060b-4ff8-b2c2-bca67eccf772",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    best_network = Network()\n",
    "    best_network.cuda()\n",
    "    best_network.load_state_dict(torch.load('/content/face_landmarks.pth')) \n",
    "    best_network.eval()\n",
    "    \n",
    "    images, landmarks = next(iter(valid_loader))\n",
    "    \n",
    "    images = images.cuda()\n",
    "    landmarks = (landmarks + 0.5) * 224\n",
    "\n",
    "    predictions = (best_network(images).cpu() + 0.5) * 224\n",
    "    predictions = predictions.view(-1,68,2)\n",
    "    \n",
    "    plt.figure(figsize=(10,40))\n",
    "    \n",
    "    for img_num in range(8):\n",
    "        plt.subplot(8,1,img_num+1)\n",
    "        plt.imshow(images[img_num].cpu().numpy().transpose(1,2,0).squeeze(), cmap='gray')\n",
    "        plt.scatter(predictions[img_num,:,0], predictions[img_num,:,1], c = 'r', s = 5)\n",
    "        plt.scatter(landmarks[img_num,:,0], landmarks[img_num,:,1], c = 'g', s = 5)\n",
    "\n",
    "print('Total number of test images: {}'.format(len(valid_dataset)))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Elapsed Time : {}\".format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
