{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7884936,"sourceType":"datasetVersion","datasetId":4628493}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# XGBoost\n\nXGBoost, short for eXtreme Gradient Boosting, is a popular machine learning library that implements the gradient boosting framework. It is designed to be highly efficient, scalable, and flexible, making it one of the most widely used algorithms for supervised learning tasks, particularly in structured/tabular data settings.\n\n### What is XGBoost?\nXGBoost belongs to the ensemble learning family, specifically boosting algorithms. Boosting is a sequential technique where the model learns from the mistakes of its predecessors and improves over time by focusing more on the misclassified data points. XGBoost extends traditional gradient boosting methods by incorporating regularization techniques and parallel processing, which significantly improves performance.\n\n### Why is XGBoost Used?\n1. <b>Performance</b>: XGBoost often outperforms other algorithms in terms of predictive accuracy. It is particularly effective for structured/tabular data where there are a large number of features.\n2. <b>Scalability</b>: XGBoost is highly scalable and can handle large datasets efficiently. It supports parallel and distributed computing, making it suitable for big data applications.\n3. <b>Flexibility</b>: It supports a variety of objective functions and evaluation metrics, allowing users to customize the model according to their specific needs.\n4. <b>Robustness</b>: XGBoost includes built-in regularization techniques to prevent overfitting, such as L1 and L2 regularization, which improve model generalization.\n\n### Advantages of XGBoost:\n1. <b>Highly Accurate</b>: XGBoost often produces state-of-the-art results on many machine learning competitions and real-world datasets.\n2. <b>Handles Missing Values</b>: XGBoost has built-in capabilities to handle missing data, reducing the need for preprocessing.\n3. <b>Feature Importance</b>: It provides feature importance scores, allowing users to interpret the model and identify important predictors.\n4. <b>Wide Range of Applications</b>: XGBoost can be applied to various machine learning tasks, including classification, regression, ranking, and recommendation systems.\n\n### Disadvantages of XGBoost:\n1. <b>Complexity</b>: The algorithm can be more complex to understand and tune compared to simpler models like linear regression or decision trees.\n2. <b>Computationally Intensive</b>: Training XGBoost models can be computationally intensive, especially for large datasets or complex models.\n3. <b>Hyperparameter Tuning</b>: While XGBoost provides many hyperparameters for tuning, finding the optimal combination can require extensive experimentation.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# XGBoost Algorithm Implementation","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:43:36.471315Z","iopub.execute_input":"2024-03-19T11:43:36.471685Z","iopub.status.idle":"2024-03-19T11:43:38.886074Z","shell.execute_reply.started":"2024-03-19T11:43:36.471656Z","shell.execute_reply":"2024-03-19T11:43:38.884894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Disable Warnings","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:43:38.888114Z","iopub.execute_input":"2024-03-19T11:43:38.888755Z","iopub.status.idle":"2024-03-19T11:43:38.893191Z","shell.execute_reply.started":"2024-03-19T11:43:38.888724Z","shell.execute_reply":"2024-03-19T11:43:38.892168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/melbourne-house-dataset/melb_house_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:43:38.894049Z","iopub.execute_input":"2024-03-19T11:43:38.894349Z","iopub.status.idle":"2024-03-19T11:43:39.008414Z","shell.execute_reply.started":"2024-03-19T11:43:38.894328Z","shell.execute_reply":"2024-03-19T11:43:39.007710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:43:39.011417Z","iopub.execute_input":"2024-03-19T11:43:39.011790Z","iopub.status.idle":"2024-03-19T11:43:39.070035Z","shell.execute_reply.started":"2024-03-19T11:43:39.011756Z","shell.execute_reply":"2024-03-19T11:43:39.068567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Statistics","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:43:39.071514Z","iopub.execute_input":"2024-03-19T11:43:39.071791Z","iopub.status.idle":"2024-03-19T11:43:39.101581Z","shell.execute_reply.started":"2024-03-19T11:43:39.071767Z","shell.execute_reply":"2024-03-19T11:43:39.100552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:43:39.102917Z","iopub.execute_input":"2024-03-19T11:43:39.103707Z","iopub.status.idle":"2024-03-19T11:43:39.161137Z","shell.execute_reply.started":"2024-03-19T11:43:39.103666Z","shell.execute_reply":"2024-03-19T11:43:39.159896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isna().sum","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:43:39.163183Z","iopub.execute_input":"2024-03-19T11:43:39.163589Z","iopub.status.idle":"2024-03-19T11:43:39.183427Z","shell.execute_reply.started":"2024-03-19T11:43:39.163556Z","shell.execute_reply":"2024-03-19T11:43:39.182727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:43:39.184531Z","iopub.execute_input":"2024-03-19T11:43:39.185706Z","iopub.status.idle":"2024-03-19T11:43:39.202543Z","shell.execute_reply.started":"2024-03-19T11:43:39.185647Z","shell.execute_reply":"2024-03-19T11:43:39.201551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.dropna(axis = 0)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:43:39.203718Z","iopub.execute_input":"2024-03-19T11:43:39.204165Z","iopub.status.idle":"2024-03-19T11:43:39.217770Z","shell.execute_reply.started":"2024-03-19T11:43:39.204144Z","shell.execute_reply":"2024-03-19T11:43:39.216789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:43:39.221225Z","iopub.execute_input":"2024-03-19T11:43:39.221711Z","iopub.status.idle":"2024-03-19T11:43:39.234579Z","shell.execute_reply.started":"2024-03-19T11:43:39.221681Z","shell.execute_reply":"2024-03-19T11:43:39.233503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Splitting","metadata":{}},{"cell_type":"code","source":"cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\nx = data[cols_to_use]","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:44:41.097892Z","iopub.execute_input":"2024-03-19T11:44:41.098287Z","iopub.status.idle":"2024-03-19T11:44:41.105224Z","shell.execute_reply.started":"2024-03-19T11:44:41.098250Z","shell.execute_reply":"2024-03-19T11:44:41.104201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = data.Price","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:44:42.073034Z","iopub.execute_input":"2024-03-19T11:44:42.073414Z","iopub.status.idle":"2024-03-19T11:44:42.077553Z","shell.execute_reply.started":"2024-03-19T11:44:42.073389Z","shell.execute_reply":"2024-03-19T11:44:42.076927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain, xtest, ytrain, ytest = train_test_split(x,\n                                               y,\n                                               test_size = 0.1)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:44:47.473332Z","iopub.execute_input":"2024-03-19T11:44:47.474542Z","iopub.status.idle":"2024-03-19T11:44:47.483578Z","shell.execute_reply.started":"2024-03-19T11:44:47.474503Z","shell.execute_reply":"2024-03-19T11:44:47.482565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Initialization","metadata":{}},{"cell_type":"code","source":"xgb1 = XGBRegressor()","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:44:48.688196Z","iopub.execute_input":"2024-03-19T11:44:48.689022Z","iopub.status.idle":"2024-03-19T11:44:48.692832Z","shell.execute_reply.started":"2024-03-19T11:44:48.688982Z","shell.execute_reply":"2024-03-19T11:44:48.691986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb1.fit(xtrain, ytrain)\nxgb1.score(xtrain, ytrain)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:44:49.193618Z","iopub.execute_input":"2024-03-19T11:44:49.194653Z","iopub.status.idle":"2024-03-19T11:44:49.366826Z","shell.execute_reply.started":"2024-03-19T11:44:49.194618Z","shell.execute_reply":"2024-03-19T11:44:49.366003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = xgb1.predict(xtest)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, ytest)))","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:45:01.812582Z","iopub.execute_input":"2024-03-19T11:45:01.812934Z","iopub.status.idle":"2024-03-19T11:45:01.825451Z","shell.execute_reply.started":"2024-03-19T11:45:01.812907Z","shell.execute_reply":"2024-03-19T11:45:01.824324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Parameter Tuning","metadata":{}},{"cell_type":"markdown","source":"XGBoost has a few features that can drastically affect the accuracy and speed of training. The first feature you need to understand are:","metadata":{}},{"cell_type":"markdown","source":"### n_estimators","metadata":{}},{"cell_type":"markdown","source":"   - Definition: `n_estimators` refers to the number of boosting rounds (decision trees) to be built in the XGBoost model.\n   - Purpose: Increasing the number of estimators typically improves the performance of the XGBoost model, as it allows for more iterations of the boosting process, leading to better fitting of the training data.\n   - Effect: However, a higher number of estimators also increases the risk of overfitting, especially if the dataset is small or noisy. It's important to find a balance where the model achieves good performance without overfitting.","metadata":{}},{"cell_type":"code","source":"xgb2 = XGBRegressor(n_estimators=500)\nxgb2.fit(xtrain, ytrain)\nxgb2.score(xtrain, ytrain)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:45:06.029060Z","iopub.execute_input":"2024-03-19T11:45:06.029797Z","iopub.status.idle":"2024-03-19T11:45:06.430519Z","shell.execute_reply.started":"2024-03-19T11:45:06.029755Z","shell.execute_reply":"2024-03-19T11:45:06.429757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### early_stopping_rounds","metadata":{}},{"cell_type":"markdown","source":"   - `early_stopping_rounds` is a technique used to prevent overfitting and improve training efficiency by stopping the training process early if the performance metric on the validation set stops improving.\n   - <b>Purpose</b>: Instead of training for a fixed number of iterations, early stopping allows the model to automatically determine the optimal number of boosting rounds based on the validation performance.\n   - <b>Effect</b>: This helps prevent overfitting and saves computational resources by avoiding unnecessary iterations. It's a crucial parameter to tune for optimizing the training process in XGBoost.","metadata":{}},{"cell_type":"code","source":"xgb3 = XGBRegressor(n_estimators=500)\nxgb3.fit(xtrain, ytrain, \n             early_stopping_rounds=5, \n             eval_set=[(xtest, ytest)],\n             verbose=False)\nxgb3.score(xtrain, ytrain)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:45:06.733784Z","iopub.execute_input":"2024-03-19T11:45:06.734206Z","iopub.status.idle":"2024-03-19T11:45:06.816292Z","shell.execute_reply.started":"2024-03-19T11:45:06.734173Z","shell.execute_reply":"2024-03-19T11:45:06.815249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### learning_rate","metadata":{}},{"cell_type":"markdown","source":"   - `learning_rate` (also known as shrinkage or eta) determines the step size at which the model weights are updated during each boosting iteration.\n   - <b>Purpose</b>: A lower learning rate makes the model training more conservative by taking smaller steps towards the optimal solution, which helps in achieving better generalization and robustness.\n   - <b>Effect</b>: However, a very low learning rate requires more boosting rounds to converge, increasing the computational cost. On the other hand, a higher learning rate may lead to faster convergence but can also cause overshooting and instability in training.","metadata":{}},{"cell_type":"code","source":"xgb4 = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nxgb4.fit(xtrain, ytrain, \n             early_stopping_rounds=5, \n             eval_set=[(xtest, ytest)],\n             verbose=False)\nxgb4.score(xtrain, ytrain)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:45:07.461552Z","iopub.execute_input":"2024-03-19T11:45:07.461932Z","iopub.status.idle":"2024-03-19T11:45:07.726475Z","shell.execute_reply.started":"2024-03-19T11:45:07.461904Z","shell.execute_reply":"2024-03-19T11:45:07.725416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### n_jobs","metadata":{}},{"cell_type":"markdown","source":"   - `n_jobs` specifies the number of parallel jobs to run during model training and prediction.\n   - <b>Purpose</b>: Setting `n_jobs` to a value greater than 1 enables parallelism, allowing XGBoost to utilize multiple CPU cores for faster computation.\n   - <b>Effect</b>: Increasing the value of `n_jobs` can significantly reduce the training and prediction time, especially for large datasets and complex models. However, it also increases the memory usage, so it's essential to consider the available resources and adjust `n_jobs` accordingly to avoid memory-related issues.","metadata":{}},{"cell_type":"code","source":"xgb5 = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\nxgb5.fit(xtrain, ytrain, \n             early_stopping_rounds=5, \n             eval_set=[(xtest, ytest)],\n             verbose=False)\nxgb5.score(xtrain, ytrain)","metadata":{"execution":{"iopub.status.busy":"2024-03-19T11:45:09.244147Z","iopub.execute_input":"2024-03-19T11:45:09.244636Z","iopub.status.idle":"2024-03-19T11:45:09.503337Z","shell.execute_reply.started":"2024-03-19T11:45:09.244601Z","shell.execute_reply":"2024-03-19T11:45:09.502641Z"},"trusted":true},"execution_count":null,"outputs":[]}]}